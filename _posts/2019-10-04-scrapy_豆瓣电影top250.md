---
layout: post
title: "初窥scrapy——爬取豆瓣电影top250"
date: 2019-10-04
description: "初窥scrapy——爬取豆瓣电影top250"
tag: python
---

### 初窥scrapy

Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。

其最初是为了 页面抓取 (更确切来说, 网络抓取 )所设计的， 也可以应用在获取API所返回的数据(例如 Amazon Associates Web Services ) 或者通用的网络爬虫。

### 爬取对象

今天我爬取的对象为豆瓣电影的top250，需要爬取的内容为：

* 电影名称
* 电影导演
* 电影年份
* 电影评分
* 电影评价人数
* 电影排名

>https://movie.douban.com/top250

![2019-10-040822121ff65.jpg](https://miao.su/images/2019/10/04/2019-10-040822121ff65.jpg)

### 程序框架
```
douban_movie/
    scrapy.cfg
    douban/
        __init__.py
        items.py
        pipelines.py
        settings.py
        spiders/
            __init__.py
            movie.py
```
**这些文件分别是:**

* scrapy.cfg: 项目的配置文件
* douban/: 该项目的python模块。之后您将在此加入代码
* douban/pipelines.py: 项目中的pipelines文件
* douban/items.py: 项目中的item文件
* douban/settings.py: 项目的设置文件
* douban/spiders/: 放置spider代码的目录

### 定义Item
`Item` 是保存爬取到的数据的容器；其使用方法和`python`字典类似， 并且提供了额外保护机制来避免拼写错误导致的未定义字段错误。

类似在`ORM`中做的一样，您可以通过创建一个 `scrapy.Item` 类， 并且定义类型为 `scrapy.Field` 的类属性来定义一个`Item`。 (如果不了解`ORM`, 不用担心，您会发现这个步骤非常简单)

首先根据需要从`dmoz.org`获取到的数据对`item`进行建模。 我们需要从`dmoz`中获取名字，`url`，以及网站的描述。 对此，在`item`中定义相应的字段。

> `items.py`代码为：

```
import scrapy
class DoubanMovieItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    ranking=scrapy.Field()
    year=scrapy.Field()
    movie_name=scrapy.Field()
    score=scrapy.Field()
    score_num=scrapy.Field()
    #电影信息
    movie_type=scrapy.Field()
    director=scrapy.Field()
```

### 提取数据
现在，我们来尝试从这些页面中提取些有用的数据。

您可以在终端中输入 `response.body` 来观察`HTML`源码并确定合适的`XPath`表达式。不过，这任务非常无聊且不易。您可以考虑使用`Firefox`的`Firebug`扩展来使得工作更为轻松。

>`movie.py`模块代码

```
# -*- coding: utf-8 -*-
import scrapy
from scrapy import Request
from douban_movie.items import DoubanMovieItem

class MovieSpider(scrapy.Spider):
    name = 'movie'
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36',
    }
#    allowed_domains = ['movie.douban.com']
#    start_urls = ['https://movie.douban.com/top250']

    def start_requests(self):
        url='https://movie.douban.com/top250'
        yield Request(url,headers=self.headers)


    def parse(self, response):
        item=DoubanMovieItem()
        movies=response.xpath('//*[@id="content"]/div/div[1]/ol/li')
        for movie in movies:
            item['ranking']=movie.xpath('.//div[@class="pic"]/em/text()').extract()[0]
            item['movie_name']=movie.xpath('.//div[@class="hd"]/a/span[@class="title"]/text()').extract()[0]
            item['score']=movie.xpath('.//div[@class="star"]/span[2]/text()').extract()[0]
            item['score_num']=movie.xpath('.//div[@class="star"]/span[4]/text()').re(u'(\d+)人评价')[0]
            item['movie_type']=movie.xpath('.//div[@class="bd"]/p/text()[2]').re(u'(\D+)\xa0/\xa0(\D+)\n')[-1]
            try:
                item['director']=movie.xpath('.//div[@class="bd"]/p/text()[1]').re(u'(导演: )(\D+)\xa0\xa0\xa0')[-1]
            except:
                item['director']="尚未匹配"
            try:
                item['year']=movie.xpath('.//div[@class="bd"]/p/text()[2]').re(u'(\d+)\xa0')[0]
            except:
                item['year']=movie.xpath('.//div[@class="bd"]/p/text()[2]').re(u'(\d+)')[0]
            yield item

        next_url=response.xpath('//span[@class="next"]/a/@href').extract()
        if next_url:
            next_url='https://movie.douban.com/top250' + next_url[0]
            yield Request(next_url,headers=self.headers)
```

### `settings.py`配置

```
FEED_EXPORT_ENCODING = 'gb18030'
ROBOTSTXT_OBEY = False
ITEM_PIPELINES = {
    'douban_movie.pipelines.DoubanMoviePipeline': 300,
}
```

<img src="https://miao.su/images/2019/08/09/9150e4e5gy1g0sab5n1uej2043037weba662a.jpg" height="100" alt="点个赞再走呗">

### 欢迎转载、留言

转载请注明出处：[小志博客](http://xiaozhi-chen.github.io) » [点击阅读原文](http://pengjuchen.tk/2019/5天破10亿的哪吒，为啥这么火？/)  
<font face="黑体" color="red">**欢迎留言**</font>

<img src="https://miao.su/images/2019/08/09/6af89bc8gw1f8qnullt9ij20140140sibd843.jpg" height="100" alt="今天我也是一个小可爱">
