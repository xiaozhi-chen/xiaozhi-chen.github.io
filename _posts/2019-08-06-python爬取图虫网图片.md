---
layout: post
title: "python爬取图虫网图片"
date: 2019-08-06
description: "python爬取图虫网图片"
tag: python
---   

### 写在前面

暑假来了，本来想着回家过来着，但是，由于需要电赛，就留在了学校。在准备电赛之余，日子过得是相当的郁闷无聊。在百无聊赖中，开始学习了python——这一门近几年炒得热火朝天的高级编程语言。我估计，在连续接下来的几篇博客都是关于python的。

### 图虫网
记得第一次接触这个图虫网，是在我做官微的时候。官微影像组里有个师兄，摄影特厉害，图虫上有很多他的作品。
> https://tuchong.com/

在跟他接触后，我也梦想着能在摄影方面有一定突破，无奈的是，相机太tm的贵了，作为一个“穷人”，实在无法拿出巨额的金钱来培养这一方面的技能。


<img src="https://miao.su/images/2019/08/09/9150e4e5ly1ftr2chzwflj20sv0skgok08fb6.jpg" height="200">


### 图虫网-爬取图虫网

为什么要爬取这个网站呢？不清楚哎……其实就是我闲得蛋疼，哈哈哈哈
百度一下`python 图虫`，发现有很多类似的文章博客，但是基本都是py2，py3的还没有人写，所以顺手写一篇吧。

### 最终效果如下

![2019-08-06_194230fde3d.jpg](https://miao.su/images/2019/08/06/2019-08-06_194230fde3d.jpg)

### 源代码

``` python
import requests
import os
import threading

img_urls=[]
lock=threading.Lock()#初始化一个锁

headers={
    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
    'accept-encoding': 'gzip, deflate, br',
    'accept-language': 'zh-CN,zh;q=0.9',
    'cookie': 'PHPSESSID=36c8n4lsbb8u63glevh1ksc9a1; webp_enabled=1; _ga=GA1.2.1167535880.1534758916; _gid=GA1.2.1330668796.1534758916; weilisessionid=aa3bf69b4f35c91ca4866315f1f300b1; wluuid=WLGEUST-02ADBA37-4B6C-DE33-2769-8697C4B575BB; wlsource=tc_pc_home; webp_enabled=0; _ga=GA1.3.1167535880.1534758916; _gid=GA1.3.1330668796.1534758916; _ba=BA0.2-20180820-51751-eyUyUL4rqUHUI1lh6uRM; qimo_seosource_e7dfc0b0-b3b6-11e7-b58e-df773034efe4=%E5%85%B6%E4%BB%96%E7%BD%91%E7%AB%99; qimo_seokeywords_e7dfc0b0-b3b6-11e7-b58e-df773034efe4=%E6%9C%AA%E7%9F%A5; accessId=e7dfc0b0-b3b6-11e7-b58e-df773034efe4; pageViewNum=1; bad_ide7dfc0b0-b3b6-11e7-b58e-df773034efe4=3c85f321-a45f-11e8-92ed-072415955da9; nice_ide7dfc0b0-b3b6-11e7-b58e-df773034efe4=3c85f322-a45f-11e8-92ed-072415955da9',
    'dnt': '1',
    'upgrade-insecure-requests': '1',
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36'
    }



class spider():
    def __init__(self,headers):
        self.headers=headers

    def get_img_urls(self,term,page):
        target_url='https://tuchong.com/rest/tags/'+term+'/posts?page='+str(page)+'&count=20&order=weekly'
        response=requests.get(target_url,headers=headers)
        postList=response.json().get('postList')
        if postList:
            print("正在解析……………………")
            for item in postList:
                url=item.get('cover_image_src')
                img_url=url.split("/")
                img_url=img_url[0]+'//'+img_url[2]+'/'+img_url[3]+'/'+'f'+'/'+img_url[5]
                img_url=img_url.replace("webp","jpg")
#               print(img_url)
                img_urls.append(img_url)


class download():
    def run(self,term):
        global img_urls
#        print("%s is running"%threading.current_thread)
        #创建路径
        path=term
        is_exists=os.path.exists(path)
        if not is_exists:
            os.makedirs(path)
            print (path+'目录创建成功')
        else:
            # 如果目录存在则不创建，并提示目录已存在
            print(path+' 目录已存在')

        while len(img_urls)>0:
            lock.acquire()
            img_url=img_urls.pop()
            lock.release()

            try:
                filename=path+"/"+img_url.split("/")[5]
                response=requests.get(img_url,headers=headers,timeout=3)
                with open(filename,'wb') as f:
                    f.write(response.content)
                    f.close()
                print(img_url.split("/")[5]+"下载成功")
            except:
                print("下载失败")


if __name__=="__main__":
    spider=spider(headers)
    term=input('输入想要搜索的内容: ')
    page=input('下载页数: ')
    for i in range(1,int(page)+1):
        spider.get_img_urls(term,i)
        #开启线程下载图片
        down=download()
        down.run(term)
```
复制粘贴即可使用，方便吧

<img src="https://miao.su/images/2019/08/09/9150e4e5gy1g0sab5n1uej2043037weba662a.jpg" height="100" alt="点个赞再走呗">

### 欢迎转载、留言

转载请注明出处：[小志博客](http://xiaozhi-chen.github.io) » [点击阅读原文](http://pengjuchen.tk/2019/08/python爬取图虫网图片/)  
<font face="黑体" color="red">**欢迎留言**</font>

<img src="https://miao.su/images/2019/08/09/6af89bc8gw1f8qnullt9ij20140140sibd843.jpg" height="100" alt="今天我也是一个小可爱">
